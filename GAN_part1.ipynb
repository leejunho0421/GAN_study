{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_part1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPoiVrFUIL9hjL9MzZgNXZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silverstar0727/GAN/blob/master/GAN_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WybckXE-gcAv",
        "colab_type": "text"
      },
      "source": [
        "# 생성 모델링이란?\n",
        "생성 모델(Generative model)의 거시적 정의: 확률 모델(probabilistic model)의 관점으로보면 생성 모델은 데이터셋을 생성하는 방법을 기술한 것. 이 모델에서의 sampling을 통해 새로운 데이터를 얻을 수 있다.\n",
        "\n",
        "## 생성모델의 과정\n",
        "훈련데이터 - 생성모델 (+ 랜덤한 잡음) - 새로운 샘플\n",
        "* 각 sample은 다양한 features로 이루어져 있다.\n",
        "* 생성모델은 결정적 (deterministic)이 아니고 확률적(probabilistic)이어야 한다 -> 랜덤한 잡음을 통해 '새로운' 샘플을 (확률적으로) 생성하도록 함\n",
        "\n",
        "## 판별 모델링(discrimizative modeling)\n",
        "훈련데이터 - 판별 모델 - 예측\n",
        "* 각 그림이 label을 가져야 함 -> supervised learning \n",
        "* 판별 모델링은 샘플 $x$가 주어졌을 때 레이블 $y$일 확률 $p(y|x)$를 추정(estimation) 하는 것이 목적\n",
        "* 생성 모델링은 $x$의 관측 확률 $p(x)$를 추정 (label이 있다면 $p(x|y)$를 추정하는 방식) -> 샘플을 발견할 확률을 추정함으로써 원본 훈련 데이터 셋에 속할 가능성이 높은 픽셀을 출력하도록 함\n",
        "\n",
        "## 생성 모델링 프레임워크\n",
        "* 샘플 데이터셋 X를 가지고 있고, 알려지지 않은 분포 $p_{data}$를 따르고 있다고 가정\n",
        "* 생성 모델 $p_{model}$이 $p_{data}$를 흉내 내도록 하고 $p_{model}$에서 샘플링한 데이터는 새로운 데이터가 됨(이때 샘플링 데이터는 $p_{data}$에 이미 존재하거나, $p_{model}$의 규칙에서 어긋나면 안됨)\n",
        "\n",
        "## 확률 개념\n",
        "Likelihood($L(\\theta|x)$): 샘플 $x$에 대해 $\\theta$의 알맞은 정도를 측정 (확률밀도함수의 함숫값과 같음, 해당 점의 발견 가능성의 문제)\n",
        "* $L(\\theta|x) = p_{\\theta}(x)$ (확률 밀도함수의 함숫값)\n",
        "* $L(\\theta|X) = \\Pi p_{\\theta}(X)$\n",
        "\n",
        "위 식은 복잡하므로 로그 가능도 (log-likelyhood)를 사용\n",
        "* $l(\\theta|X) = \\Sigma logp_\\theta(x)$\n",
        "\n",
        "Maximum likelihood estimation(최대우도함수 추정): 확률밀도함수 $p_{\\theta}(x)$의 파라미터 집합$\\theta$에서 관측된 데이터 X를 가장 잘 설명할 수 있는 추정값을 찾는 과정\n",
        "\n",
        "* $ \\hat{\\theta} = argmax L(\\theta|X)$\n",
        "\n",
        "## 패션 조합의 예제\n",
        "50개의 패션 샘플에서 가능한 특정 파라미터(조합벡터)의 최대 가능도는 다음과 같이 계산된다.$$\\widehat{\\theta_j} = {{n_j}\\over{N}} $$ 여기서 $n_j$는 발견된 조합의 횟수이다. 따라서, 50개의 데이터 중 2개가 특정 파라미터에 해당했다면 그 최대 가능도는 $2\\over50$으로 정해질 것이다.\n",
        "\n",
        "그러나 원본 데이터 셋에 없는 조합의 최대 가능도는 0이기 때문에 새로운 샘플의 생성에 반영할 수 없다. 따라서 가능한 특성의 조합에 가상의 횟수 1을 더하도록 한다. 이를 가법 평화(addictive smoothing)이라고 한다. $$\\widehat{\\theta_j} = {n_j + 1 \\over N + d}$$이다.\n",
        "\n",
        "## 나이브 베이즈(Naive Bayes)\n",
        "나이브 베이즈 모델은 간단한 가정을 통해 추정할 파라미터 수를 크게 줄여준다.\n",
        "-> 각 특성 $x_j$가 모든 특성 $x_k$에 독립적이라고 가정하자\n",
        "* 즉, $p(x_j | x_k) = p(x_j)$이고 이는 나이브 베이즈 가정이라 함\n",
        "* 정리하면 $p(x) = \\Pi p(k_x)$\n",
        "* 결국 각 특성에 대해 파라미터 $\\theta_{kl} = p(x_k = l)$을 독립적으로 추정하는 문제로 귀결\n",
        "* 최대 가능도 추정값 $\\widehat{\\theta_{kl}} = {n_{kl}\\over N}$\n",
        "\n",
        "여기서 $widehat{theta_{kl}}$은 데이터셋에서 특성 k의 값이 1인  횟수를 의미함\n",
        "따라서, 특정 샘플을 생성하는 확률을 계산하기 위해서는 모든 특성의 확률($\\hat{\\theta}$)을 곱하면 된다!!!\n",
        "\n",
        " BUT.... 픽셀단위의 추정에서는 조건부 의존성이 커서 베이즈 가정이 성립하지 않음 -> 딥러닝을 통해 해결\n",
        "\n",
        "## 표현학습(representation learning)\n",
        "고차원 이미지를 다루기 힘들기 때문에 저차원 잠재공간으로 일종의 표현을 해야 함\n",
        "-> 특성을 가장 잘 추출할 수 있는 데이터의 비선형 매니폴드를 찾고 공간을 설명하기 위해 필요한 차원을 구성해야 함 -> 해당 책의 기본이 됨\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_stPUXUeFfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}